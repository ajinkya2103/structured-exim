{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.2.4 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (0.2.4)\n",
      "Requirement already satisfied: langchain-openai==0.1.8 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (0.1.8)\n",
      "Requirement already satisfied: llmwhisperer-client==0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: pydantic==2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (2.7.4)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.0.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain==0.2.4->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain==0.2.4->-r requirements.txt (line 1)) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain==0.2.4->-r requirements.txt (line 1)) (3.10.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.6 in /opt/anaconda3/lib/python3.12/site-packages (from langchain==0.2.4->-r requirements.txt (line 1)) (0.2.43)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain==0.2.4->-r requirements.txt (line 1)) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/anaconda3/lib/python3.12/site-packages (from langchain==0.2.4->-r requirements.txt (line 1)) (0.1.145)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain==0.2.4->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain==0.2.4->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain==0.2.4->-r requirements.txt (line 1)) (8.2.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai==0.1.8->-r requirements.txt (line 2)) (1.55.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai==0.1.8->-r requirements.txt (line 2)) (0.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic==2.7.4->-r requirements.txt (line 4)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic==2.7.4->-r requirements.txt (line 4)) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic==2.7.4->-r requirements.txt (line 4)) (4.11.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.4->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.4->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.4->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.4->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.4->-r requirements.txt (line 1)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.4->-r requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.6->langchain==0.2.4->-r requirements.txt (line 1)) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.6->langchain==0.2.4->-r requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.4->-r requirements.txt (line 1)) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.4->-r requirements.txt (line 1)) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.4->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.26.0->langchain-openai==0.1.8->-r requirements.txt (line 2)) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.26.0->langchain-openai==0.1.8->-r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.26.0->langchain-openai==0.1.8->-r requirements.txt (line 2)) (0.7.1)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.26.0->langchain-openai==0.1.8->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.26.0->langchain-openai==0.1.8->-r requirements.txt (line 2)) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.2.4->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.2.4->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.2.4->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.2.4->-r requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai==0.1.8->-r requirements.txt (line 2)) (2024.9.11)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.4->-r requirements.txt (line 1)) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.4->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.6->langchain==0.2.4->-r requirements.txt (line 1)) (2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pytesseract import image_to_string\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pypdfium2 as pdfium\n",
    "import streamlit as st\n",
    "import multiprocessing\n",
    "from tempfile import NamedTemporaryFile\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "from tempfile import NamedTemporaryFile\n",
    "from jsonformer.format import highlight_values\n",
    "from jsonformer.main import Jsonformer\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub.inference_api import InferenceApi\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, \\\n",
    "    HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from unstract.llmwhisperer.client import LLMWhispererClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/lib/python3.12/site-packages (0.26.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerAddress(BaseModel):\n",
    "    zip_code: str = Field(description=\"Should contain the zip code alone\")\n",
    "    city: str = Field(description=\"Should hold the city name from the address\")\n",
    "    full_address: str = Field(\n",
    "        description=\"Should hold the full address of the customer\")\n",
    "\n",
    "\n",
    "class PaymentInfo(BaseModel):\n",
    "    due_date: datetime = Field(description=\"The due date of the credit card statement. Also known as the payment due \"\n",
    "                                           \"date\")\n",
    "    minimum_payment: float = Field(\n",
    "        description=\"the minimum amount that is due\")\n",
    "    new_balance: float = Field(\n",
    "        description=\"the total new balance amount that can be paid\")\n",
    "\n",
    "\n",
    "class SpendLineItem(BaseModel):\n",
    "    spend_date: datetime = Field(description=\"The date of the transaction. If the year part isn't mentioned in the \"\n",
    "                                             \"line item explicitly, pick up the year from the statement date and use \"\n",
    "                                             \"it instead.\")\n",
    "    spend_description: str = Field(description=\"The description of the spend\")\n",
    "    amount: float = Field(description=\"The amount of the transaction\")\n",
    "\n",
    "\n",
    "class ParsedCreditCardStatement(BaseModel):\n",
    "    issuer_name: str = Field(description=\"What is the name of the issuer or the bank who has issued this credit card? \"\n",
    "                                         \"I am not interested in the legal entity, but the primary brand name of the \"\n",
    "                                         \"credit card.\")\n",
    "    customer_name: str = Field(description=\"What is the name of the customer to whom this credit card statement \"\n",
    "                                           \"belongs to? Format the name of the customer well with the first letter of \"\n",
    "                                           \"each name capitalized.\")\n",
    "    customer_address: CustomerAddress = Field(description=\"Since there might be multiple addresses in the context \"\n",
    "                                                          \"provided to you, first gather all addresses. Try to \"\n",
    "                                                          \"understand whom this credit card statement is being \"\n",
    "                                                          \"addressed to or in other words, the name of the customer. \"\n",
    "                                                          \"Find the address that matches that person's. Be sure to \"\n",
    "                                                          \"return the customer's address, for whom this credit card \"\n",
    "                                                          \"statement is for. Do not respond with any other address.\")\n",
    "    payment_info: PaymentInfo = Field(description=\"Payment information is important part of any credit card statement \"\n",
    "                                                  \"and it consists of the new balance or the full amount due for the \"\n",
    "                                                  \"current statement, the minimum payment due and the payment due \"\n",
    "                                                  \"date.\")\n",
    "    spend_line_items: list[SpendLineItem] = Field(description=\"This credit card statement contains spending details \"\n",
    "                                                              \"line items. Spend details can be split across the \"\n",
    "                                                              \"provided context. Respond with details of all the \"\n",
    "                                                              \"spend items by looking at the whole context always.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_llm_whisperer_call(file_path):\n",
    "    print(f\"Processing file:{file_path}...\")\n",
    "    # LLMWhisperer API key is picked up from the environment variable\n",
    "    client = LLMWhispererClient()\n",
    "    result = client.whisper(file_path=file_path,\n",
    "                            processing_mode=\"ocr\", output_mode=\"line-printer\")\n",
    "    return result[\"extracted_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cache_file_name(file_path):\n",
    "    # For our use case, PDFs won't be less than 4096, practically speaking.\n",
    "    if os.path.getsize(file_path) < 4096:\n",
    "        error_exit(\"File too small to process.\")\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        first_block = f.read(4096)\n",
    "        # seek to the last block\n",
    "        f.seek(-4096, os.SEEK_END)\n",
    "        f.read(4096)\n",
    "        last_block = f.read(4096)\n",
    "\n",
    "    first_md5_hash = hashlib.md5(first_block).hexdigest()\n",
    "    last_md5_hash = hashlib.md5(last_block).hexdigest()\n",
    "    return f\"/tmp/{first_md5_hash}_{last_md5_hash}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_file_cached(file_path):\n",
    "    cache_file_name = generate_cache_file_name(file_path)\n",
    "    cache_file = Path(cache_file_name)\n",
    "    if cache_file.is_file():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(file_path):\n",
    "    if is_file_cached(file_path):\n",
    "        print(f\"Info: File {file_path} is already cached.\")\n",
    "        cache_file_name = generate_cache_file_name(file_path)\n",
    "        with open(cache_file_name, \"r\") as f:\n",
    "            return f.read()\n",
    "    else:\n",
    "        data = make_llm_whisperer_call(file_path)\n",
    "        cache_file_name = generate_cache_file_name(file_path)\n",
    "        with open(cache_file_name, \"w\") as f:\n",
    "            f.write(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_exit(error_message):\n",
    "    print(error_message)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_usage_and_exit():\n",
    "    error_exit(\"Please pass name of directory or file to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerate_pdf_files(file_path):\n",
    "    files_to_process = []\n",
    "    # Users can pass a directory or a file name\n",
    "    if os.path.isfile(file_path):\n",
    "        if os.path.splitext(file_path)[1][1:].strip().lower() == 'pdf':\n",
    "            files_to_process.append(file_path)\n",
    "    elif os.path.isdir(file_path):\n",
    "        print(\"Under the is directory else case\")\n",
    "        files = os.listdir(file_path)\n",
    "        for file_name in files:\n",
    "            print(\"This is one of the fileName \", file_name)\n",
    "            full_file_path = os.path.join(file_path, file_name)\n",
    "            print(\"This is the full_file_path \", full_file_path)\n",
    "\n",
    "            if os.path.isfile(full_file_path):\n",
    "                print(\"Yes it is the file\")\n",
    "                if os.path.splitext(file_name)[1][1:].strip().lower() == 'pdf' or os.path.splitext(file_name)[1][1:].strip().lower() == 'png':\n",
    "                    files_to_process.append(full_file_path)\n",
    "    else:\n",
    "        error_exit(f\"Error. {file_path} should be a file or a directory.\")\n",
    "\n",
    "    return files_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values_from_file(raw_file_data):\n",
    "    preamble = (\"\\n\"\n",
    "                \"Your ability to extract and summarize this information accurately is essential for effective \"\n",
    "                \"credit card statement analysis. Pay close attention to the credit card statement's language, \"\n",
    "                \"structure, and any cross-references to ensure a comprehensive and precise extraction of \"\n",
    "                \"information. Do not use prior knowledge or information from outside the context to answer the \"\n",
    "                \"questions. Only use the information provided in the context to answer the questions.\\n\")\n",
    "    postamble = \"Do not include any explanation in the reply. Only include the extracted information in the reply.\"\n",
    "    system_template = \"{preamble}\"\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(\n",
    "        system_template)\n",
    "    human_template = \"{format_instructions}\\n{raw_file_data}\\n{postamble}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        human_template)\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=ParsedCreditCardStatement)\n",
    "    print(\"HAHAH\", parser.get_format_instructions())\n",
    "\n",
    "    # compile chat template\n",
    "    chat_prompt = ChatPromptTemplate.from_messages(\n",
    "        [system_message_prompt, human_message_prompt])\n",
    "    request = chat_prompt.format_prompt(preamble=preamble,\n",
    "                                        format_instructions=parser.get_format_instructions(),\n",
    "                                        raw_file_data=raw_file_data,\n",
    "                                        postamble=postamble).to_messages()\n",
    "    print(\"This is the request \", request)\n",
    "    model = ChatOpenAI()\n",
    "    print(\"Querying model...\")\n",
    "    result = model(request, temperature=0)\n",
    "    print(\"Response from model:\")\n",
    "    print(result.content)\n",
    "    return result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_files(file_list):\n",
    "    for file_path in file_list:\n",
    "        raw_file_data = extract_text(file_path)\n",
    "        print(f\"Extracted text for file {file_path}:\\n{raw_file_data}\")\n",
    "        extracted_json = extract_values_from_file(raw_file_data)\n",
    "        json_file_path = f\"{file_path}.json\"\n",
    "        with open(json_file_path, \"w\") as f:\n",
    "            f.write(extracted_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    load_dotenv()\n",
    "    if len(sys.argv) < 2:\n",
    "        show_usage_and_exit()\n",
    "\n",
    "    print(f\"Processing path {sys.argv[1]}...\")\n",
    "    file_list = enumerate_pdf_files(sys.argv[1])\n",
    "    print(f\"Processing {len(file_list)} files...\")\n",
    "    print(f\"Processing first file: {file_list[0]}...\")\n",
    "    process_pdf_files(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the is directory else case\n",
      "This is one of the fileName  chase_cc.png\n",
      "This is the full_file_path  /Users/ajshinde/Work/structured-extraction-main/assets/imgs/chase_cc.png\n",
      "Yes it is the file\n",
      "['/Users/ajshinde/Work/structured-extraction-main/assets/imgs/chase_cc.png']\n"
     ]
    }
   ],
   "source": [
    "print(enumerate_pdf_files('/Users/ajshinde/Work/structured-extraction-main/assets/imgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: File /Users/ajshinde/Work/structured-extraction-main/assets/imgs/chase_cc.png is already cached.\n"
     ]
    }
   ],
   "source": [
    "txt = extract_text(\n",
    "    '/Users/ajshinde/Work/structured-extraction-main/assets/imgs/chase_cc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n                                           Manage your account online at:       Customer Service:      Mobile: Download the \\n                                ®          www.chase.com/cardhelp                1-800-524-3880         Chase Mobile app today \\n           freedom \\n\\n\\n                                    New Balance \\n          February 2024                                         CHASE FREEDOM: ULTIMATE \\n   S   M    T   W    T   F    S      $5,084.29 \\n                                                                REWARDS®              SUMMARY \\n                                     Minimum Payment Due \\n   28 29    30 31    1    2   3                                  Previous points balance                           40,468 \\n                                     $50.00                                                                         5,085 \\n   4    5   6    7   8    9   10                                 + 1% (1 Pt)/$1 earned on all purchases \\n                                     Payment Due Date \\n   11 12    13 14    15 16    17                                 Total points available for \\n                                     02/28/24 \\n   18 19    20 21    22 23    24                                 redemption                                   45,553 \\n   25 26    27 28    29   1   2                                 Start redeeming today. Visit Ultimate Rewards@ at \\n                                                                www.ultimaterewards.com \\n   3   4    5   6    7    8   9 \\n\\n\\n                                                                You always earn unlimited 1% cash back on all your purchases. \\nLate Payment Warning: If we do not receive your minimum payment Activate new bonus categories every quarter. You'll earn an \\nby the date listed above, you may have to pay a late fee of up to additional 4% cash back, for a total of 5% cash back on up to \\n$40.00 and your APR's will be subject to increase to a maximum $1,500 in combined bonus category purchases each quarter. \\nPenalty APR of 29.99%.                                          Activate for free at chase.com/freedom, visit a Chase branch or \\n                                                                call the number on the back of your card. \\nMinimum Payment Warning: If you make only the minimum \\npayment each period, you will pay more in interest and it will take you \\nlonger to pay off your balance. For example: \\n\\n\\n    If you make no     You will pay off the And you will end up \\n   additional charges balance shown on this paying an estimated \\n  using this card and statement in about ...   total of ... \\n each month you pay ... \\n\\n\\n  Only the minimum         15 years            $12,128 \\n       payment \\n\\n\\n        $189                3 years             $6,817 \\n                                           (Savings=$5,311) \\n\\n\\nIf you would like information about credit counseling services, call \\n1-866-797-2885. \\n\\n\\n ACCOUNT SUMMARY \\nAccount Number: 4342 3780 1050 7320 \\nPrevious Balance                                 $4,233.99 \\nPayment, Credits                                 -$4,233.99 \\nPurchases                                        +$5,084.29 \\nCash Advances                                        $0.00 \\nBalance Transfers                                    $0.00 \\nFees Charged                                         $0.00 \\nInterest Charged                                     $0.00 \\nNew Balance                                      $5,084.29 \\nOpening/Closing Date                     01/04/24 - 02/03/24 \\nCredit Access Line                                 $31,700 \\nAvailable Credit                                   $26,615 \\nCash Access Line                                    $1,585 \\nAvailable for Cash                                  $1,585 \\n  Past Due Amount                                    $0.00 \\n  Balance over the Credit Access Line                $0.00 \\n\\n\\n YOUR ACCOUNT MESSAGES \\n\\n\\nReminder: It is important to continue making your payments on time. Your APRs may increase if the minimum payment is not made on \\ntime or payments are returned. \\n\\n\\nYour next AutoPay payment for $5,084.29 will be deducted from your Pay From account and credited on your due \\ndate. If your due date falls on a Saturday, we'll credit your payment the Friday before. \\n\\n\\n0000001 FIS33339 D 12                  Y 9 03 24/02/03         Page 1 of 3       06610 MA MA 34942   03410000120003494201 \\n0404 \\n\\n\\n           freedom®                       43323770106074170000500000508429000000002 \\n\\n\\n         P.O. BOX 15123 \\n                                                                Payment Due Date:                                02/28/24 \\n  WILMINGTON, DE 19850-5123           AUTOPAY IS ON \\n                                      See Your Account          New Balance:                                    $5,084.29 \\n    For Undeliverable Mail Only \\n                                     Messages for details. \\n                                                                Minimum Payment Due:                               $50.00 \\n                                                                           Account number: 4342 3780 1050 7320 \\n\\n\\n                                                                  $                                          Amount Enclosed \\n            34942 BEX 9 03424 D                                                        AUTOPAY IS ON \\n            LARRY PAGE \\n            C PAGE \\n            24917 KEYSTONE AVE \\n            LOS ANGELES CA 97015-5505 \\n                                                                                      CARDMEMBER SERVICE \\n                                                                                      PO BOX 6294 \\n                                                                                      CAROL STREAM IL 60197-6294 \\n\\n\\n                          ⑆500016028⑆32370106074177⑈ [ ]             [ ] \\n<<<\\n\\x0c\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Extract structured info from text via LLM\n",
    "\n",
    "\n",
    "class HuggingFaceLLM:\n",
    "    def __init__(self, temperature=0, top_k=50, model_name=\"databricks/dolly-v2-12b\"):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, use_cache=True, device_map=\"auto\",offload_folder=\"offload\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name, use_fast=True, use_cache=True,offload_folder=\"offload\")\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def generate(self, prompt, max_length=1024):\n",
    "        json = {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"items\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"description\": {\"type\": \"string\"},\n",
    "                            \"price\": {\"type\": \"number\"}\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"Company_name\": {\"type\": \"string\"},\n",
    "                \"invoice_date\": {\"type\": \"string\"},\n",
    "            }\n",
    "        }\n",
    "\n",
    "        builder = Jsonformer(\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            json_schema=json,\n",
    "            prompt=prompt,\n",
    "            max_string_token_length=20\n",
    "        )\n",
    "\n",
    "        print(\"Generating...\")\n",
    "        output = builder()\n",
    "        # highlight_values(output)\n",
    "        # print(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_structured_data(content: str, data_points):\n",
    "    # Choose the desired Hugging Face model\n",
    "    llm = HuggingFaceLLM(temperature=0)\n",
    "\n",
    "    template = \"\"\"\n",
    "    You are an expert admin people who will extract core information from documents\n",
    "\n",
    "    {content}\n",
    "\n",
    "    Above is the content; please try to extract all data points from the content above:\n",
    "    {data_points}\n",
    "    \"\"\"\n",
    "\n",
    "    # Fill in the placeholders in the template\n",
    "    formatted_template = template.format(\n",
    "        content=content, data_points=data_points)\n",
    "    # print(formatted_template)\n",
    "\n",
    "    # Generate text using the formatted template\n",
    "    print(\"Before the llm.genetate\")\n",
    "    results = llm.generate(formatted_template)\n",
    "    print(\"After the llm.genetate\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_data_points = \"\"\"{\n",
    "        \"item\": [{\n",
    "            \"description\": \"description or name of the item that has been bougth\",\n",
    "            \"price\": \"how much does the item cost\"\n",
    "        }],\n",
    "        \"Company_name\": \"company that issued the invoice\",\n",
    "        \"invoice_date\": \"when was the invoice issued\",\n",
    "    }\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/anaconda3/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.26.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.6.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before the llm.genetate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 35.81 GB, other allocations: 318.89 MB, max allowed: 36.27 GB). Tried to allocate 315.09 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m extract_structured_data(txt,default_data_points)\n",
      "Cell \u001b[0;32mIn[22], line 21\u001b[0m, in \u001b[0;36mextract_structured_data\u001b[0;34m(content, data_points)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(formatted_template)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Generate text using the formatted template\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBefore the llm.genetate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m results \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mgenerate(formatted_template)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter the llm.genetate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Cell \u001b[0;32mIn[21], line 40\u001b[0m, in \u001b[0;36mHuggingFaceLLM.generate\u001b[0;34m(self, prompt, max_length)\u001b[0m\n\u001b[1;32m     31\u001b[0m builder \u001b[38;5;241m=\u001b[39m Jsonformer(\n\u001b[1;32m     32\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     33\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     max_string_token_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m output \u001b[38;5;241m=\u001b[39m builder()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# highlight_values(output)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# print(output)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/jsonformer/main.py:242\u001b[0m, in \u001b[0;36mJsonformer.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 242\u001b[0m     generated_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_object(\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjson_schema[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generated_data\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/jsonformer/main.py:147\u001b[0m, in \u001b[0;36mJsonformer.generate_object\u001b[0;34m(self, properties, obj)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, schema \u001b[38;5;129;01min\u001b[39;00m properties\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[generate_object] generating value for\u001b[39m\u001b[38;5;124m\"\u001b[39m, key)\n\u001b[0;32m--> 147\u001b[0m     obj[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_value(schema, obj, key)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/jsonformer/main.py:178\u001b[0m, in \u001b[0;36mJsonformer.generate_value\u001b[0;34m(self, schema, obj, key)\u001b[0m\n\u001b[1;32m    176\u001b[0m     new_array \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    177\u001b[0m     obj[key] \u001b[38;5;241m=\u001b[39m new_array\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_array(schema[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m], new_array)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m schema_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    180\u001b[0m     new_obj \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/jsonformer/main.py:199\u001b[0m, in \u001b[0;36mJsonformer.generate_array\u001b[0;34m(self, item_schema, obj)\u001b[0m\n\u001b[1;32m    197\u001b[0m obj\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m    198\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(input_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 199\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(input_tensor\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m    200\u001b[0m logits \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    203\u001b[0m top_indices \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;241m30\u001b[39m)\u001b[38;5;241m.\u001b[39mindices\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:1178\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;124;03m    Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;124;03m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1178\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt_neox(\n\u001b[1;32m   1179\u001b[0m     input_ids,\n\u001b[1;32m   1180\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1181\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1182\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1183\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1184\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1185\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1186\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1187\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1188\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1189\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1190\u001b[0m )\n\u001b[1;32m   1192\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1193\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_out(hidden_states)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:951\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    938\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    939\u001b[0m         layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    940\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m         position_embeddings,\n\u001b[1;32m    949\u001b[0m     )\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 951\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m layer(\n\u001b[1;32m    952\u001b[0m         hidden_states,\n\u001b[1;32m    953\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m    954\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    955\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask[i],\n\u001b[1;32m    956\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    957\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    958\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    959\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    960\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    961\u001b[0m     )\n\u001b[1;32m    962\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:704\u001b[0m, in \u001b[0;36mGPTNeoXLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, use_cache, layer_past, output_attentions, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    694\u001b[0m     hidden_states: Optional[torch\u001b[38;5;241m.\u001b[39mFloatTensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    702\u001b[0m     position_embeddings: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# will become mandatory in v4.46\u001b[39;00m\n\u001b[1;32m    703\u001b[0m ):\n\u001b[0;32m--> 704\u001b[0m     attention_layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[1;32m    705\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states),\n\u001b[1;32m    706\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    707\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    708\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[1;32m    709\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    710\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    711\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    712\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    713\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    714\u001b[0m     )\n\u001b[1;32m    715\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attention_layer_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: attn_output, present, (attn_weights)\u001b[39;00m\n\u001b[1;32m    716\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_dropout(attn_output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/supply_chain/lib/python3.12/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:487\u001b[0m, in \u001b[0;36mGPTNeoXSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, layer_past, use_cache, output_attentions, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m causal_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m q_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[1;32m    488\u001b[0m     query\u001b[38;5;241m=\u001b[39mquery,\n\u001b[1;32m    489\u001b[0m     key\u001b[38;5;241m=\u001b[39mkey,\n\u001b[1;32m    490\u001b[0m     value\u001b[38;5;241m=\u001b[39mvalue,\n\u001b[1;32m    491\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m    492\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout\u001b[38;5;241m.\u001b[39mp \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    493\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m    494\u001b[0m )\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# Reshape outputs\u001b[39;00m\n\u001b[1;32m    497\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 35.81 GB, other allocations: 318.89 MB, max allowed: 36.27 GB). Tried to allocate 315.09 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "extract_structured_data(txt,default_data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ajshinde/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "ajinkyashinde\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
